{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93404a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650a242",
   "metadata": {},
   "source": [
    "# Requesting the table of content page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9166958",
   "metadata": {},
   "source": [
    "First we need to download the page containing all links for songs for an artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3691bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.lyrics.com/artist/James-Vincent-McMorrow/1093507','https://www.lyrics.com/artist/Mumford-%26-Sons/1570809']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbfd6610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mumford_&_Sons'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_artist_name(url):\n",
    "    artist_name =  url.split('/')[4].replace(\"-\", \"_\")\n",
    "    artist_name = artist_name.replace('%26', '&')\n",
    "    return artist_name\n",
    "\n",
    "get_artist_name('https://www.lyrics.com/artist/Mumford-%26-Sons/1570809')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b661cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_artist_page(url):\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "    r=requests.get(url, headers=headers)\n",
    "    text = r.text\n",
    "    soup = BeautifulSoup(text, 'html')\n",
    "    artist_name = get_artist_name(url)\n",
    "    #artist = soup.h1.text.replace(\" \", \"_\")\n",
    "    #artist_name = str(artist)\n",
    "    #artist_name = artist_name.replace('%26', '&')\n",
    "    #print(artist_name)\n",
    "\n",
    "    with open (str(artist_name) + '_lyrics_page', 'w') as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    \"\"\"with open ('james_vincent_mcorrow_lyrics_page', 'r') as f:\n",
    "        my_string = f.read()\n",
    "    print(my_string)\"\"\"\n",
    "    \n",
    "    #print(f)\n",
    "    return soup\n",
    "    \n",
    "#download_artist_page('https://www.lyrics.com/artist/Mumford-%26-Sons/1570809')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93415a2a",
   "metadata": {},
   "source": [
    "Then we explore the page with beautiful soup to find all links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a8bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(soup):\n",
    "    page_links = soup.find_all('a')\n",
    "    song_links = []\n",
    "    song_names = []\n",
    "    for link in page_links: \n",
    "        if '/lyric/' in link['href']:\n",
    "            split_name = link.text.split(\"[\")[0].strip()\n",
    "            if split_name not in song_names:\n",
    "                song_links.append(link['href'])\n",
    "                song_names.append(split_name)\n",
    "            \n",
    "    print(\"Song links found\")\n",
    "    print(\"___________\")\n",
    "    print(\"\")\n",
    "    return song_links\n",
    "\n",
    "#get_links(download_artist_page(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d077f",
   "metadata": {},
   "source": [
    "We then prepare a function to extract the lyrics out of the lyrics page and clean up any style or tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5be67a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(url):\n",
    "    soup_lyrics = BeautifulSoup(requests.get(url).text, 'html')\n",
    "    lyrics = soup_lyrics.find_all(id='lyric-body-text')\n",
    "\n",
    "    for tag in lyrics:\n",
    "        for minitag in tag:\n",
    "            if minitag in tag.find_all('a'):\n",
    "                del minitag['href']\n",
    "                del minitag['style']\n",
    "                minitag.extract()\n",
    "    return lyrics[0].get_text().replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e82b5a",
   "metadata": {},
   "source": [
    "We can then use that function inside a container function that can iterate through all the links returned by our get_links() function and use get_lyrics to write the lyrics into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27da4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lyrics(song_links):\n",
    "    print('Starting lyrics downloads')\n",
    "    for counter, link in enumerate(song_links[:6]):\n",
    "        url = 'https://www.lyrics.com' + link\n",
    "        response = requests.get(url)\n",
    "        #print(response)\n",
    "        \n",
    "        artist = link.split('/')[3].replace(\"+\", \"_\")\n",
    "        artist = artist.replace('%26', '&')\n",
    "        # I somehow can't decode the special characters in the song titles, \n",
    "        #so I'll just replace some of the common ones\n",
    "        \n",
    "        song_title = link.split('/')[4].replace(\"+\", \" \")\n",
    "        song_title = song_title.replace(\"%27\", \"'\")\n",
    "        song_title = song_title.replace('%26', \"&\")\n",
    "        song_title = song_title.replace('%2C', ',')\n",
    "        song_title = song_title.replace('%C3%A9', 'Ã©')\n",
    "        song_title = song_title.replace('%5B', '[')\n",
    "        song_title = song_title.replace('%5D', ']')\n",
    "        song_title = song_title.replace('%21', '!')\n",
    "        song_title = song_title.replace('%28', '(')\n",
    "        song_title = song_title.replace('%29', ')')\n",
    "        \n",
    "        # now we write all the lyrics into an artist txt file! We use the append mode \n",
    "        #to add lyrics so the last lyrics don't get overwritten\n",
    "        #return artist\n",
    "        \n",
    "        with open(f'lyrics/{artist}.txt', 'a') as f:\n",
    "            f.write(\"\\n\" + get_lyrics(url))\n",
    "        print(counter, song_title, response.status_code)\n",
    "        #return artist\n",
    "        \n",
    "\n",
    "#links_list = get_links(soup)\n",
    "#write_lyrics(links_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240f136",
   "metadata": {},
   "source": [
    "From there we can create a function to clean up the lyrics documents we just made to prepare them for the bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b138060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_lyrics(artist):\n",
    "    with open ('lyrics/' + artist +'.txt', 'r') as f:\n",
    "        my_lyrics = f.read()\n",
    "    my_lyrics = my_lyrics.split('\\n')\n",
    "    my_lyrics = [s.lower() for s in my_lyrics]\n",
    "\n",
    "    clean_lyrics = []\n",
    "\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for doc in my_lyrics:\n",
    "        #print(doc)\n",
    "        tokens = tokenizer.tokenize(text=doc)\n",
    "        clean_doc = \" \".join(lemmatizer.lemmatize(token) for token in tokens)\n",
    "        clean_lyrics.append(clean_doc)\n",
    "    labels_one_artist = [artist] * len(clean_lyrics)\n",
    "    global labels\n",
    "    labels.extend(labels_one_artist)\n",
    "    global corpus\n",
    "    corpus.extend(clean_lyrics)    \n",
    "\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d144673",
   "metadata": {},
   "source": [
    "That's it! Now we create our execution function so we can launch everything from one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b508c674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now looking at James_Vincent_McMorrow\n",
      "Song links found\n",
      "___________\n",
      "\n",
      "Starting lyrics downloads\n",
      "0 I Lie Awake Every Night 200\n",
      "1 Alone Together 200\n",
      "2 Evil 200\n",
      "3 One Thousand Times 200\n",
      "4 The Future 200\n",
      "5 True Care 200\n",
      "\n",
      "_____________\n",
      "\n",
      "Now looking at Mumford_&_Sons\n",
      "Song links found\n",
      "___________\n",
      "\n",
      "Starting lyrics downloads\n",
      "0 Beloved 200\n",
      "1 Blind Leading the Blind 200\n",
      "2 Devil in Your Eye 200\n",
      "3 The Boxer 200\n",
      "4 42 200\n",
      "5 Guiding Light 200\n",
      "\n",
      "_____________\n",
      "\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "corpus = []\n",
    "\n",
    "def run_all(url):\n",
    "    \n",
    "    artists = []\n",
    "    artist = get_artist_name(url)\n",
    "    print('Now looking at ' + artist)\n",
    "    \n",
    "    soup = download_artist_page(url)\n",
    "    links_list = get_links(soup)\n",
    "    write_lyrics(links_list)\n",
    "    artists.append(artist)\n",
    "    clean_up_lyrics(artist)\n",
    "    print()\n",
    "    print(\"_____________\")\n",
    "    print()\n",
    "        \n",
    "for url in urls:\n",
    "    run_all(url)\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bad3a2",
   "metadata": {},
   "source": [
    "For debugging and clarity I'm putting all the corpus and the labels into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9844fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tuples = list(zip(corpus,labels))\n",
    "df = pd.DataFrame(data_tuples, columns=['corpus','labels'])\n",
    "#df.head(20)\n",
    "#df.tail(20)\n",
    "#df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f8856",
   "metadata": {},
   "source": [
    "I want to calculate the accuracy of my predictions, so I'm going to train_test_split to create a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a009b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['corpus']\n",
    "y = df['labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eebba94",
   "metadata": {},
   "source": [
    "We can now calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af445acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7837837837837838"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_score():\n",
    "    \n",
    "    stopwords_ = stopwords.words('english')\n",
    "\n",
    "    steps = [('tf-idf', TfidfVectorizer(stop_words=stopwords_)),\n",
    "              ('LR', LogisticRegression(class_weight='balanced'))\n",
    "            ]\n",
    "\n",
    "    pipeline = Pipeline(steps)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    pipeline.predict(X_test)\n",
    "    #pipeline.predict_proba(X_test)\n",
    "    return pipeline.score(X_test, y_test)\n",
    "    \n",
    "    #return pipeline.predict([text])[-1]\n",
    "\n",
    "calculate_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb6153f",
   "metadata": {},
   "source": [
    "And finally predict the artist for new texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def predict_text(text):\n",
    "    \n",
    "    stopwords_ = stopwords.words('english')\n",
    "\n",
    "    steps = [('tf-idf', TfidfVectorizer(stop_words=stopwords_)),\n",
    "              ('LR', LogisticRegression(class_weight='balanced'))\n",
    "            ]\n",
    "\n",
    "    pipeline = Pipeline(steps)\n",
    "    pipeline.fit(corpus, labels)\n",
    "    pipeline.predict([text])\n",
    "    pipeline.predict_proba([text])\n",
    "    \n",
    "    return pipeline.predict([text])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#string = 'your love is gold'\n",
    "string = input(\"Enter your string:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mmmmmh I think this text is from')\n",
    "predict_text(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
